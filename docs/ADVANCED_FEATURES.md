# CodeWave Advanced Features

Deep dive into CodeWave's sophisticated analysis capabilities: Developer Overview generation, Convergence Detection, and Multi-Round Agent Discussion.

---

## Developer Overview

Intelligent, AI-generated summary of code changes created automatically before agent evaluation.

### What is Developer Overview?

A concise, human-readable summary of what changed in a commit, generated by analyzing the diff:

```
Summary: Added actual estimation as a separate step

Details:
Introduced actual time estimation alongside ideal time in PR analysis
for better accuracy.

Key Changes:
- Implemented IActualTimeEstimator interface
- Created ActualTimeRunnable for estimation
- Merged actual time with PR lifecycle data
```

### How It Works

1. **Automatic Generation**: Runs as first step in evaluation pipeline
2. **Multi-Stage Process**:
   - Extract key changes from diff
   - Identify files modified and their purpose
   - Generate concise technical summary
   - Format for readability

3. **No Agent Opinion**: This is factual analysis, not evaluation
   - Not influenced by agent assessments
   - Same output regardless of agent disagreements
   - Based purely on code diff

4. **Available Everywhere**:
   - HTML report card (top section)
   - results.json file (`developerOverview` field)
   - Used as context for all agents

### When Generation Might Fail

**Common Causes:**

- Very large diffs (>500KB) - may timeout
- Binary files in commit - can't be analyzed
- Corrupted file content
- Network issues during generation

**What Happens:**

- Report shows: "Overview generation failed"
- Agents still evaluate using raw diff
- Report remains complete and useful
- Check console logs with `--verbose` for details

### Using Developer Overview

**For Team Leads:**

```
Quick commit assessment without reading full diff
1. Scan Developer Overview summary (10 seconds)
2. Check Functional Impact score
3. Decide if detailed review needed
```

**For CI/CD Pipelines:**

```bash
# Extract overview
jq '.developerOverview' results.json

# Parse for specific info
jq '.developerOverview' results.json | grep -i "breaking change"
```

**For Documentation:**

```bash
# Generate change log
for file in .evaluated-commits/*/results.json; do
  echo "## $(jq -r '.metadata.commitMessage' "$file")"
  echo "$(jq -r '.developerOverview' "$file")"
  echo ""
done > CHANGELOG_GENERATED.md
```

---

## Convergence Detection

Sophisticated algorithm that measures consensus among agents and optimizes evaluation rounds.

### What is Convergence?

**Definition**: How closely agents' final scores agree with each other

**Score Range**: 0.0 to 1.0 (higher = better consensus)

**Interpretation**:

- **0.9+**: Excellent consensus, very reliable evaluation
- **0.7-0.8**: Good consensus, minor disagreements
- **0.5-0.6**: Moderate agreement, some conflicting views
- **<0.5**: Low consensus, significant disagreement

### How Convergence Detection Works

**Phase 1: Calculate Metric Variance**

```
For each 7-pillar metric:
1. Get final score from each agent
2. Calculate standard deviation
3. Normalize to 0-1 scale (0 = all same, 1 = max variance)

Example:
Code Quality scores: [7, 7, 8, 6, 7] â†’ variance = 0.2 (low)
Complexity scores: [3, 5, 7, 4, 6] â†’ variance = 0.8 (high)
```

**Phase 2: Weighted Aggregation**

```
Convergence = 1 - (weighted_average_variance)

Weights:
- Code Quality: 2x (critical for quality gate)
- Test Coverage: 2x (critical for reliability)
- Functional Impact: 1.5x (important)
- Others: 1x (supporting metrics)
```

**Phase 3: Dynamic Round Optimization**

```
Target: Convergence >= 0.75 (configurable)

If convergence < target:
  â†’ Continue to next round
  â†’ Agents see previous assessments
  â†’ Can adjust scores with reasoning

Maximum rounds: 3 (avoid infinite loops)

When to stop:
âœ“ Convergence reached target
âœ“ No new gaps identified
âœ“ Maximum rounds reached
```

### Convergence in Action

**Low Convergence Example:**

```
Evaluation 1 - Initial Assessments:
- Code Quality: [7, 6, 8] â†’ Convergence: 0.45 (low)
- Variance: Reviewer thinks 7, Architect thinks 6, different perspectives

Round 2 - Concerns Raised:
- Architect: "Code is well-written but tightly coupled"
- Reviewer: "Coupling is acceptable for this context"
- Revised scores: [7, 7, 8] â†’ Convergence: 0.65 (improving)

Round 3 - Validation:
- All agents review feedback
- Final scores: [7, 7, 7] â†’ Convergence: 0.92 (excellent)
```

**High Convergence Example:**

```
Evaluation 1 - Initial Assessment:
- All agents strongly agree on scores
- Code Quality: [8, 8, 8] â†’ Convergence: 0.98
- No disagreements to discuss
- Stop after Round 1 âœ“
```

### Using Convergence Scores

**For Quality Assurance:**

```bash
# Find unreliable evaluations (low convergence)
jq '.[] | select(.convergenceScore < 0.6)' \
  .evaluated-commits/*/history.json
```

**For Statistical Analysis:**

```bash
# Average convergence across commits
jq '[.[].convergenceScore] | add / length' \
  .evaluated-commits/*/history.json
```

**For Flagging Problem Commits:**

```bash
# If convergence drops, something unusual about commit
jq -r 'select(.convergenceScore < 0.5) | .evaluationNumber' \
  .evaluated-commits/*/history.json
```

### Convergence Insights

**What Low Convergence Tells You:**

1. Commit is unusual or controversial
2. Different aspects are in tension (e.g., high impact but high risk)
3. Worth extra review and discussion
4. Disagreement is valuable - multiple perspectives on real trade-offs

**What High Convergence Tells You:**

1. Straightforward change
2. Clear quality assessment
3. Reliable evaluation
4. Agents quickly reached consensus

---

## Multi-Round Agent Discussion

The core mechanism that makes CodeWave's evaluation sophisticated: agents discuss, challenge, and refine their assessments.

### How Multi-Round Discussion Works

#### Round 1: Initial Assessment

**Agent Activity:**

- Each agent evaluates independently
- Uses only: diff, metadata, developer overview
- No knowledge of other agents' scores
- Each provides: summary, details, metrics

**Example Output:**

```
Business Analyst ðŸŽ¯
Summary: High-impact feature for user workflows
Metrics: Impact=8, Ideal Time=6 hours

SDET ðŸ§ª
Summary: Test coverage needs improvement for edge cases
Metrics: Test Coverage=5, (no impact on other scores)

Developer Reviewer ðŸ”
Summary: Code well-structured but naming unclear in helpers
Metrics: Code Quality=6
```

**Purpose of Round 1:**

- Establish baseline perspectives
- Identify areas of agreement/disagreement
- Set context for discussion

#### Round 2: Concerns & Refinement

**What Happens:**

1. System collects all Round 1 scores
2. Agents see **only their own assessment** plus the **aggregated consensus**
3. System formulates concerns: "Impact scored 8 but Code Quality only 6 - why?"
4. Agents review concerns and can revise scores

**Agent Activity in Round 2:**

```
Architect sees:
- Their score: Complexity = 4
- Average from all: Complexity = 5.2
- Question: "Why is architectural complexity higher in consensus?"

Architect reviews feedback:
"Developer Reviewer noted tight coupling in data layer"

Architect revises:
"I was too optimistic. Coupling does increase complexity.
Revising: Complexity = 6 (was 4), Tech Debt = +3 hours"
```

**Purpose of Round 2:**

- Challenge assumptions
- Incorporate peer feedback
- Move toward consensus
- Highlight real trade-offs

#### Round 3: Validation & Consensus

**What Happens:**

1. System shows updated scores from Round 2
2. Agents review changes and final metrics
3. Final opportunity to adjust or validate
4. System calculates convergence

**Agent Activity in Round 3:**

```
All agents review updated scores
Are new assessments reasonable? Yes âœ“
Do they reflect actual concerns? Yes âœ“
Final validation accepted
Convergence Score: 0.78 âœ“ (meets target)
```

**Purpose of Round 3:**

- Final validation
- Ensure changes are justified
- Build consensus
- Prepare final report

### What Each Agent Evaluates

#### Round 1-3: Business Analyst ðŸŽ¯

**Primary Metrics:**

- Functional Impact (1-10): User/business value
- Ideal Time (hours): Perfect-case implementation time

**Questions in Discussions:**

- "Is this business value real or perceived?"
- "Are requirements clear enough for accurate time estimation?"
- "Does scope match the effort?"

**Concerns Raised:**

- Scope creep: "Impact high but ideal time seems short"
- Unclear requirements: "Can't estimate if requirements are vague"
- Misalignment: "High complexity but low impact seems risky"

---

#### Round 1-3: Developer Author ðŸ‘¨â€ðŸ’»

**Primary Metrics:**

- Actual Time (hours): Real time spent
- Tech Debt (hours): Debt added (+) or removed (-)

**Questions in Discussions:**

- "What obstacles were encountered?"
- "Is the implementation approach sustainable?"
- "Did architectural choices increase debt?"

**Concerns Raised:**

- Time overrun: "Actual time 3x ideal - significant complexity"
- Debt accumulation: "Implementation created architectural liabilities"
- Maintainability: "Technical choices make future changes harder"

---

#### Round 1-3: Developer Reviewer ðŸ”

**Primary Metrics:**

- Code Quality (1-10): Correctness, design, readability
- Secondary: Test coverage adequacy

**Questions in Discussions:**

- "Does the code work correctly?"
- "Are there design improvements?"
- "Is it maintainable?"

**Concerns Raised:**

- Design issues: "Violates DRY principle, creates duplication"
- Readability: "Complex logic needs comments"
- Testing: "Core logic lacks test coverage"

---

#### Round 1-3: Senior Architect ðŸ›ï¸

**Primary Metrics:**

- Complexity (1-10, reversed): How simple/complex is the architecture?
- Tech Debt (hours): Architectural liabilities

**Questions in Discussions:**

- "Is this scalable?"
- "Does it follow SOLID principles?"
- "What long-term consequences?"

**Concerns Raised:**

- Scalability: "Direct database access won't scale to production"
- Coupling: "Too tightly coupled to external service"
- Patterns: "Violates established architectural patterns"

---

#### Round 1-3: SDET ðŸ§ª

**Primary Metrics:**

- Test Coverage (1-10): Comprehensiveness of testing
- Quality: Automation framework quality

**Questions in Discussions:**

- "Are we confident this works?"
- "What scenarios aren't tested?"
- "Is test code maintainable?"

**Concerns Raised:**

- Coverage gaps: "Happy path tested but error handling not"
- Fragile tests: "Tests too tightly coupled to implementation"
- Infrastructure: "Test framework doesn't support edge case scenarios"

---

### Why Multi-Round Discussion Improves Evaluation

**Problem with Single-Round Evaluation:**

```
Agent 1 sees: "This is high impact"
Agent 2 sees: "This has high complexity"
Agent 3 sees: "Test coverage is low"

Without discussion:
â†’ High impact wins, low quality issues missed
â†’ Recommendation: "Ship it" (wrong!)
```

**With Multi-Round Discussion:**

```
Round 1: All agents submit assessments
Round 2: System highlights: "High impact but low quality - contradiction?"
Round 3: Agents discuss and agree: "High impact, but ship only with test improvements"
â†’ Recommendation: "Ship with test coverage requirement" (better!)
```

### Multi-Round Discussion in HTML Report

**View the Timeline:**

1. Open HTML report
2. Click "Conversation Timeline" section
3. See each round's assessments
4. Understand how agents refined their thinking

**What to Look For:**

- **Consensus**: When all agents agree (stronger signals)
- **Debate**: Different perspectives highlight real trade-offs
- **Reasoning**: Why scores changed between rounds

### Practical Examples

**Example 1: Quality vs. Speed Trade-off**

```
Round 1:
- Business Analyst: "High impact, implement quickly" (Impact=8, Ideal=4)
- SDET: "Test coverage is poor" (Tests=3)
- Architect: "Code is brittle" (Quality=4)

Round 2 Discussion:
- Analyst: "Should we delay for quality improvements?"
- Architect: "Yes, technical debt isn't worth 4-hour savings"

Round 3 Result:
- Recommendation: Add 2 more hours for quality
- Impact remains 8 (value is there)
- Quality improves to 7
- Ideal Time revised to 6 (was 4)
```

**Example 2: Framework Decision**

```
Round 1:
- Architect: "New framework adds complexity" (Complexity=3)
- SDET: "Framework lacks test support" (Tests=4)
- Developer: "Learning curve cost 6 hours" (Actual=10)

Round 2 Discussion:
- Architect: "Long-term benefits reduce complexity later?"
- Developer: "Yes, but current evaluation shows short-term pain"
- SDET: "Maturity improves next quarter"

Round 3 Result:
- Acknowledged: Current project is learning investment
- Tech Debt: +3 hours (but will reduce as team learns)
- Recommendation: Accept trade-off for strategic benefits
```

---

## Understanding Evaluation Depth

### Shallow Evaluation (Converges Quickly)

```
Round 1 Scores: [8, 8, 8, 8, 8]
Convergence: 0.98 âœ“

Typical: Simple bug fixes, obvious improvements
Time: Fast (single round)
Reliability: Very high (unanimous)
```

### Deep Evaluation (Multiple Rounds)

```
Round 1: [8, 5, 6, 4, 7] â†’ Convergence: 0.35 (low)
Round 2: [7, 6, 6, 5, 7] â†’ Convergence: 0.62 (improving)
Round 3: [7, 6, 7, 6, 7] â†’ Convergence: 0.78 (good) âœ“

Typical: Complex features, trade-offs
Time: Longer (multiple rounds)
Reliability: Refined through discussion
Detail: Rich with nuance
```

### Key Insight

**More rounds â‰  worse evaluation**

Sometimes high-complexity changes need multiple rounds to properly assess trade-offs. The convergence score tells you if agents reached meaningful consensus.

---

## Advanced Usage

### Analyzing Discussion Quality

```bash
# Find commits with most agent disagreement
jq -s 'sort_by(.convergenceScore) | .[0:5]' \
  .evaluated-commits/*/history.json
```

### Identifying Red Flags

```bash
# Low convergence suggests need for discussion
jq '.[] | select(.convergenceScore < 0.6) | .evaluationNumber' \
  .evaluated-commits/*/history.json
```

### Tracking Consensus Over Time

```bash
# Graph convergence trend
for eval in .evaluated-commits/*/history.json; do
  echo "$(basename $(dirname $eval)): $(jq '.[-1].convergenceScore' $eval)"
done | sort -t: -k2 -rn
```

---

## Troubleshooting

### Convergence Very Low (<0.3)

**Possible Causes:**

1. Highly ambiguous commit
2. Major architectural disagreement
3. Unclear requirements

**What to Do:**

- Review agent feedback carefully
- Look for specific concerns in conversation
- Consider follow-up PR to address issues

### Convergence Stays Low Across Multiple Evaluations

**Possible Causes:**

1. Commit is genuinely controversial
2. Multiple valid interpretations
3. System may need recalibration

**What to Do:**

- This is valuable signal! Agents keep finding issues
- Treat as "needs team discussion"
- Low convergence is honest evaluation

---

## Next Steps

- **[HTML_REPORT_GUIDE.md](./HTML_REPORT_GUIDE.md)** - Learn to read the report
- **[AGENTS.md](./AGENTS.md)** - Detailed agent specifications
- **[ARCHITECTURE.md](./ARCHITECTURE.md)** - System implementation details
